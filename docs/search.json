[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "",
    "text": "Topic modeling in Natural Language Processing (NLP) is a technique used to discover hidden themes or topics within a collection of text documents. It’s an unsupervised machine learning technique, meaning it doesn’t require predefined tags or training data that’s been previously classified by humans. The main objective of topic modeling is to discover topics that are clusters of words expressed as a combination of strongly related words.\nOne popular algorithm for topic modeling is Latent Dirichlet Allocation (LDA). Topic modeling is used in various applications such as chatbots, autocorrection, speech recognition, language translation, social media monitoring, hiring and recruitment, email filtering, and more.\n\n\nLatent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data such as text corpora. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. The topic proportions of a document are assumed to have a Dirichlet prior. The topic-specific word distributions also have a Dirichlet prior.\n\n\n\nLDA Diagram, credit:Think Infi\n\n\nLDA has these key assumptions:\n\nDocuments are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\nDocuments are exchangeable.\n\nRequirements for the data:\n\nEach document must represent a mixture of topics.\nEach word must be generated from a single topic.\n\n\n\n\n\nLDA is designed for long documents, but tweets are short.\nLDA assumes that each document is a mixture of topics, but tweets are often about a single topic.\nShort texts from social network platform has other characteristics that are not considered by LDA, such as users, hashtags, mentions, etc.\n\n\n\n\n\nMerge all short documents by the same users into a long document\nUse a single topic for each short document\n\nPer-shot-text LDA (aka. Twitter-LDA)\nDirichlet-Multinomial Mixture (DMM)\n\nLearn topics from longer documents (e.g., news articles) and apply them to short texts\nClassify shot texts utilizing neural networks\n\nThe word mover’s distance (WMD)\nWord embeddings\n\nClustering techniques\n\nHowever, while there are methods available for analyzing short-text documents, they do have some limitations. Specifically, these methods do not retain user information and co-occurrence of words within the same short texts. Additionally, they require a corpus of long text that is already compatible with the short-text documents, and relying on pre-trained word embeddings may not accurately reflect the specific vocabulary and semantic usage of words in the short texts."
  },
  {
    "objectID": "index.html#introduction-topic-modeling-and-lda",
    "href": "index.html#introduction-topic-modeling-and-lda",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "",
    "text": "Topic modeling in Natural Language Processing (NLP) is a technique used to discover hidden themes or topics within a collection of text documents. It’s an unsupervised machine learning technique, meaning it doesn’t require predefined tags or training data that’s been previously classified by humans. The main objective of topic modeling is to discover topics that are clusters of words expressed as a combination of strongly related words.\nOne popular algorithm for topic modeling is Latent Dirichlet Allocation (LDA). Topic modeling is used in various applications such as chatbots, autocorrection, speech recognition, language translation, social media monitoring, hiring and recruitment, email filtering, and more.\n\n\nLatent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data such as text corpora. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. The topic proportions of a document are assumed to have a Dirichlet prior. The topic-specific word distributions also have a Dirichlet prior.\n\n\n\nLDA Diagram, credit:Think Infi\n\n\nLDA has these key assumptions:\n\nDocuments are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\nDocuments are exchangeable.\n\nRequirements for the data:\n\nEach document must represent a mixture of topics.\nEach word must be generated from a single topic.\n\n\n\n\n\nLDA is designed for long documents, but tweets are short.\nLDA assumes that each document is a mixture of topics, but tweets are often about a single topic.\nShort texts from social network platform has other characteristics that are not considered by LDA, such as users, hashtags, mentions, etc.\n\n\n\n\n\nMerge all short documents by the same users into a long document\nUse a single topic for each short document\n\nPer-shot-text LDA (aka. Twitter-LDA)\nDirichlet-Multinomial Mixture (DMM)\n\nLearn topics from longer documents (e.g., news articles) and apply them to short texts\nClassify shot texts utilizing neural networks\n\nThe word mover’s distance (WMD)\nWord embeddings\n\nClustering techniques\n\nHowever, while there are methods available for analyzing short-text documents, they do have some limitations. Specifically, these methods do not retain user information and co-occurrence of words within the same short texts. Additionally, they require a corpus of long text that is already compatible with the short-text documents, and relying on pre-trained word embeddings may not accurately reflect the specific vocabulary and semantic usage of words in the short texts."
  },
  {
    "objectID": "index.html#introducing-the-stlda-c-model",
    "href": "index.html#introducing-the-stlda-c-model",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "Introducing the stLDA-C Model",
    "text": "Introducing the stLDA-C Model\nThe stLDA-C model was proposed by Tierney et al. in their paper “Author Clustering and Topic Estimation for Short Texts.” This model particularly aims to improve topic estimation in brief documents, such as social media posts, and incorporates the grouping of authors for more effective analysis.\nstLDA features:\n\nShort text LDA topic model with unsupervised clustering of authors of short documents - Fusing the clustering of both authors(users) and documents\nHierarchical model capalbe of sharing information at multiple levels leading to higher quality estimates of per-author topic distributions, per-cluster topic distribution centers, and author cluster assignments.\n\nThe stLDA-C model is specifically designed to handle the sparsity of words in short texts by considering the additional structure provided by user clusters and potentially by integrating external information or employing different priors that are more suitable for short texts.\n\nWhat’s new in the stLDA-C model?\nTo understand what’s new in the stLDA-C model, let’s first take a closer look at the traditional LDA model.\n\n\n\n\n\n\nTraditional LDA\n\n\n\n\n\n\n\nstLDA-C\n\n\n\n\n\nQuick summary of the traditional LDA notations:\nW: Word\nZ: Topic\nLDA Input:\n\nM number of documents\nEach of these documents have N number of words\n\nLDA Output:\n\nK number of topics (cluster of words)\nΦ distribution (document to topic distribution)\n\nCompared with the traditional LDA, the stLDA-C model adds a layer of user clustering and a layer of hierarchical topic distributions. From the diagrams, we can see that the stLDA-C model introduced several changes and additions:\n\nThe model considers \\(G\\) clusters of users, where \\(G\\) is a hyperparameter.\n\\(G_u\\) represents the assignment of each user to a specific cluster, governed by the \\(\\phi\\) parameter.\n\\(\\alpha_g\\) is the vector parameter of a Dirichlet distribution over topics choices for users in cluster \\(g\\).\n\\(\\phi\\) represents the distribution over user clusters. It forms a prior for the user cluster assignments. In traditional LDA, there is no concept of user clusters, so this parameter \\(\\nu\\) is specific to stLDA. \\(\\phi\\) encodes the proportion of users in each group and forms a prior distribution for \\(Gu\\).\n\\(\\theta_u\\): Because the model assumes that each document (tweet) is generated by a single topic, the consideration for the document-topic distribution is replaced by user-topic distribution. Each user-specific topic distribution \\(\\theta_u\\) is a draw from \\(Dir(\\alpha_g)\\), where \\(g\\) is the cluster assignment of user \\(u\\).\n\\(Z_{ud}\\) is the topic of each tweet \\(d\\) by user \\(u\\). \\(Z_{ud}\\) is a single draw from \\(\\theta_u\\), and all words in tweet \\(ud\\) are sampled from the topic distribution over words, \\(\\beta_t\\), where \\(Z_{ud} = t\\).\n\nThe generative process of the stLDA-C model is as follows:\n\n\n\nGenerative process of the stLDA-C model\n\n\n\n\nTL;DR\nVery intimidating, right? Let’s break it down:\n\n\n\nstLDA-C model workflow\n\n\nThree key takeaways from the stLDA model:\n\nUser Clustering: stLDA clusters users by topic preferences, enhancing the analysis of datasets where authorship is significant.\nHierarchical Topic Distributions: The model employs hierarchical priors for nuanced cluster-level and user-specific topic analysis.\nIntegrated Topic-User and Word Analysis: stLDA combines topic-user dynamics with word co-occurrence for comprehensive short text analysis."
  },
  {
    "objectID": "index.html#the-stlda-c-model-in-action-analyze-us-congress-members-tweets",
    "href": "index.html#the-stlda-c-model-in-action-analyze-us-congress-members-tweets",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "The stLDA-C Model in Action: Analyze US Congress members’ tweets",
    "text": "The stLDA-C Model in Action: Analyze US Congress members’ tweets\nWe gathered tweets from U.S. Congress members posted from August 16 to September 26, 2020. Our focus was on the 60 most active Twitter users among them, specifically selecting their top 20 tweets based on the highest combined counts of likes and retweets. Among these 60 Congress members, 37 are Democrats and 22 are Republicans.\n\nVisualize the networks of tweets and users\nFirst, we will visualize the networks of tweets and users. The nodes in the network are the tweets and users, and the edges are the co-occurrence of words in the tweets. The size of the nodes is proportional to the betweeness centrality of the nodes (in the following plot, users). Here we presenting a static and an interactive version of the network of users.\n\n\nCode\n# Create the networks\ntweets_w &lt;- PrepText(top_tweets, groupvar = \"screen_name\", textvar = \"text_com\", node_type = \"words\", tokenizer = \"words\", pos = \"nouns\", remove_stop_words = TRUE, compound_nouns = FALSE)\ntweets_g &lt;- PrepText(top_tweets, groupvar = \"screen_name\", textvar = \"text_com\", node_type = \"groups\", tokenizer = \"words\", pos = \"nouns\", remove_stop_words = TRUE, compound_nouns = FALSE)\n\ntweets_w_nw &lt;- CreateTextnet(tweets_w)\ntweets_g_nw &lt;- CreateTextnet(tweets_g)\n\n# Save the networks to local files\nsaveRDS(tweets_w_nw, \"rcs/data/tweets_w_nw.rds\")\nsaveRDS(tweets_g_nw, \"rcs/data/tweets_g_nw.rds\")\n\n# # ## Check the distribution of degree of the nodes\ndegree &lt;- degree(tweets_w_nw)\nhist(degree, breaks = 100, main = \"Degree Distribution of Words\", xlab = \"Degree\")\n\n\n\n\n\nCode\n# ## Check number of nodes and edges\n# vcount(tweets_w_nw)\n# ecount(tweets_w_nw)\n# vcount(tweets_g_nw)\n# ecount(tweets_g_nw)\n\nVisTextNet(tweets_g_nw, alpha = 0.25, label_degree_cut=10, betweenness=TRUE)\n\n\n\n\n\nCode\nVisTextNetD3(tweets_g_nw, alpha = 0.2, charge=-50,zoom = TRUE)\n\n\n\n\n\n\nThe network visualization reveals that the 40 users are grouped into six clusters. However, these groupings do not necessarily align with the cluster estimates provided by the stLDA-C model.\n\n\nCode\n## Get a subset from \"tweets_w\" of 10 users with 10 words with highest degree\n# select the top 5 lemmas for each user\ntop_lemmas_per_user &lt;- tweets_w %&gt;%\n  group_by(screen_name) %&gt;%\n  slice_max(order_by = count, n = 10, with_ties = FALSE)\n\n# This example selects the top 20 users based on the total count of their lemmas\ntop_users &lt;- top_lemmas_per_user %&gt;%\n  group_by(screen_name) %&gt;%\n  summarise(total_count = sum(count)) %&gt;%\n  arrange(desc(total_count)) %&gt;%\n  slice_head(n = 25) %&gt;%\n  ungroup()\n\n# Finally, subset the original top lemmas dataset to only include these top users\nfinal_subset &lt;- top_lemmas_per_user %&gt;%\n  filter(screen_name %in% top_users$screen_name)\n\n\n\n## Copy tweets_w and rename \"screen_name\" to \"groupvar\" and \"text_com\" to \"textvar\"\ntweets_data &lt;- final_subset %&gt;%\n  rename(groupvar = screen_name) %&gt;%select(groupvar, lemma)\n\n\n# Create edges between outlets and lemmatized words\nedges_outlet_word &lt;- tweets_data %&gt;%\n  select(groupvar, lemma) %&gt;%\n  distinct()\n\n# Convert to edge list and create a graph\nedge_list &lt;- as.data.frame(edges_outlet_word)\ng &lt;- graph_from_data_frame(edge_list, directed = FALSE)\n\n# Set type attribute\nV(g)$type &lt;- ifelse(V(g)$name %in% tweets_data$groupvar, TRUE, FALSE)\n\n# Convert to networkD3 format and create interactive plot\nnetwork_data &lt;- igraph_to_networkD3(g)\nnetwork_data$nodes$group &lt;- ifelse(network_data$nodes$name %in% tweets_data$groupvar, \"Outlet\", \"Word\")\n\n# Interactive plot\nforceNetwork(Links = network_data$links, Nodes = network_data$nodes,\n             Source = \"source\", Target = \"target\",\n             NodeID = \"name\", Group = \"group\", \n             zoom = TRUE, fontSize = 30, charge=-30,\n             colourScale = JS(\"d3.scaleOrdinal().range(['#76b7b2', '#f28e2b'])\"))\n\n\n\n\n\n\nThis interactive plot shows the network of the top 10 words and top 25 users with the highest degree. The nodes in blue represent the top 10 users, and the nodes in orange represent the top 10 words. This plot has nothing to do with the topic modeling, but simply shows how the users and their words conduct a 2-mode or bipartite network. The stLDA-C model is able to capture the distribution of topics in this network.\n\n\nUse stLDA-C to analyze US Senators’ tweets\nNow, we’ll try to use the stLDA-C model to analyze the tweets of the 40 senators. We will use the same code as the authors of the stLDA-C model provided in their demo. We set the number of topics to 6, which is the same as the number of clusters we found in the network visualization. The number of user clusters is set to 2, indicating the party affiliation of the congress members.\n\n\nCode\n#######################\n### Visualizations ####\n#######################\n\n#print top 15 words from each topic\ngroundtruth_estimate[[\"tw\"]] %&gt;% \n  top_topic_words(words = words,n=15) %&gt;% \n  t\n\n\n      [,1]       [,2]       [,3]       [,4]       [,5]       [,6]      \n [1,] \"campaign\" \"campaign\" \"american\" \"campaign\" \"campaign\" \"american\"\n [2,] \"country\"  \"care\"     \"campaign\" \"care\"     \"country\"  \"care\"    \n [3,] \"court\"    \"country\"  \"care\"     \"country\"  \"court\"    \"country\" \n [4,] \"election\" \"days\"     \"country\"  \"court\"    \"election\" \"court\"   \n [5,] \"first\"    \"election\" \"court\"    \"election\" \"first\"    \"election\"\n [6,] \"get\"      \"first\"    \"election\" \"first\"    \"get\"      \"first\"   \n [7,] \"going\"    \"get\"      \"first\"    \"get\"      \"going\"    \"get\"     \n [8,] \"law\"      \"going\"    \"get\"      \"help\"     \"help\"     \"ginsburg\"\n [9,] \"must\"     \"health\"   \"help\"     \"must\"     \"must\"     \"going\"   \n[10,] \"police\"   \"help\"     \"must\"     \"police\"   \"police\"   \"health\"  \n[11,] \"right\"    \"right\"    \"november\" \"right\"    \"right\"    \"help\"    \n[12,] \"today\"    \"thank\"    \"right\"    \"supreme\"  \"supreme\"  \"really\"  \n[13,] \"voting\"   \"today\"    \"today\"    \"today\"    \"today\"    \"right\"   \n[14,] \"want\"     \"voting\"   \"want\"     \"want\"     \"want\"     \"voting\"  \n[15,] \"women\"    \"want\"     \"women\"    \"women\"    \"women\"    \"women\"   \n\n\nCode\n#print cluster means with user-level topic estimates overlayed\n#grey bars are cluster-level expected values, colored lines are each user's topic distribution\n#note that clusters with 1 user do not visualize well\n\n# Extract estimated cluster assignments from the model results\nca_est &lt;- groundtruth_estimate[[\"ca\"]]  %&gt;% results_freq_table() %&gt;% apply(1, which.max)\n\n# The following line is commented out because ca_true doesn't exist in your actual data scenario\n# table(ca_est, ca_true)\n\n# Function to plot clusters\nplot_clusters &lt;- function(ut_mat, cluster_assignment, cluster_alphas, yRange = c(0, .5)) {\n  cluster_means &lt;- cluster_alphas %&gt;% {./rowSums(.)}\n  ut_mat &lt;- ut_mat %&gt;% {./rowSums(.)}\n  \n  lapply(unique(cluster_assignment), function(c) {\n    ut_mat %&gt;%\n    {.[cluster_assignment == c, ]} %&gt;%\n      t %&gt;%\n      data.frame(Topic = 1:ncol(ut_mat), .) %&gt;%\n      reshape2::melt(id.vars = \"Topic\") %&gt;%\n      ggplot(aes(x = Topic, y = value)) +\n      geom_line(aes(color = variable)) +\n      guides(color = \"none\") +\n      geom_bar(data = data.frame(x = 1:ncol(ut_mat), y = cluster_means[c, ]), aes(x = x, y = y), alpha = .5, stat = \"identity\") +\n      labs(title = str_c(\"Cluster \", c, \" (n=\", sum(cluster_assignment == c), \")\"), y = \"Probability\") +\n      ylim(yRange)\n  })\n}\n\n# Generate and arrange cluster plots\nclusterPlots &lt;- plot_clusters(ut_mat = groundtruth_estimate[[\"ut\"]] %&gt;% results_array_mean(),\n                              cluster_assignment = groundtruth_estimate[[\"ca\"]] %&gt;% results_freq_table() %&gt;% apply(1, which.max),\n                              cluster_alphas = groundtruth_estimate[[\"alphag\"]] %&gt;% results_array_mean())\n\nclusterPlots %&gt;% gridExtra::grid.arrange(grobs = .)\n\n\n\n\n\n\n\nConclusion\nUnfortunately, the stLDA-C model did not perform as well as we had hoped. The key words for the topics were not distinguishable enough to be useful, and the cluster assignment did not work on the 60 users. (For a ideal result, please refer back to the workflow diagram above.)\nDespite the advanced capabilities of the short text Latent Dirichlet Allocation (stLDA) model, obtaining ideal results from the analysis of 1200 tweets from 60 users proved challenging, primarily due to the inherent sparsity and brevity of tweets. It will take some trial and error to determine the optimal number of topics and clusters to use in the model.\nAdditionally, the effectiveness of stLDA is highly sensitive to text preprocessing choices and hyperparameter settings, which require meticulous tuning. To improve future analyses, a more extensive dataset could be beneficial, alongside a refined approach to preprocessing and an iterative process of parameter optimization to better capture the nuances of short text data."
  },
  {
    "objectID": "index.html#explore-more",
    "href": "index.html#explore-more",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "Explore more:",
    "text": "Explore more:\n\nWhat is Topic Modeling? Definition, Uses, & Examples\nWhat is Topic Modeling? A Beginner’s Guide\nTopic Modeling: An Introduction - MonkeyLearn\nTopic Modelling | Topic Modelling in Natural Language Processing.\nA Beginner’s Guide to Latent Dirichlet Allocation(LDA)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]