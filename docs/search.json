[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "",
    "text": "LDA is a generative probabilistic model for collections of discrete data such as text corpora.\nDocuments, Words, Topics: LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics.\nAssumptions:\n\nDocuments are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\nDocuments are exchangeable.\nEach word’s creation is conditioned on a single topic.\nThe topic proportions of a document are assumed to have a Dirichlet prior.\nThe topic-specific word distributions also have a Dirichlet prior.\n\nRequirements for the data:\n\nEach document must represent a mixture of topics.\nEach word must be generated from a single topic.\n\n\n\n\n\n\nLDA is designed for long documents, but tweets are short.\nLDA assumes that each document is a mixture of topics, but tweets are often about a single topic.\nShort texts from SSN has other characteristics that are not considered by LDA, such as users, hashtags, mentions, etc.\n\n\n\n\n\nMerge all short documents by the same users into a long document\nUse a single topic for each short document\n\nPer-shot-text LDA (aka. Twitter-LDA)\nDirichlet-Multinomial Mixture (DMM)\n\nLearn topics from longer documents (e.g., news articles) and apply them to short texts\nClassify shot texts utilizing neural networks\n\nThe word mover’s distance (WMD)\nWord embeddings\n\nClustering techniques\n\n\n\n\n\nThe user information is lost\nThe co-occurence of words within the same short texts is lost\nRequires a corpus of long text that is a priori compatible with the short-text documents\nRely on pre-trained word embeddings may not reflect the specific voabulary and semantic usage of words in the short texts\n\n\n\n\n\nShort text LDA topic model with unsupervised clustering of authors of short documents\nFuses the clustering of both authors(users) and documents\nThis hierachical model is able to share information at multiple levels leading to higher quality estimates of per-author topic distributions, per-cluster topic distribution centers, and autho cluster assignments."
  },
  {
    "objectID": "index.html#part-1-introduction",
    "href": "index.html#part-1-introduction",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "",
    "text": "LDA is a generative probabilistic model for collections of discrete data such as text corpora.\nDocuments, Words, Topics: LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics.\nAssumptions:\n\nDocuments are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\nDocuments are exchangeable.\nEach word’s creation is conditioned on a single topic.\nThe topic proportions of a document are assumed to have a Dirichlet prior.\nThe topic-specific word distributions also have a Dirichlet prior.\n\nRequirements for the data:\n\nEach document must represent a mixture of topics.\nEach word must be generated from a single topic.\n\n\n\n\n\n\nLDA is designed for long documents, but tweets are short.\nLDA assumes that each document is a mixture of topics, but tweets are often about a single topic.\nShort texts from SSN has other characteristics that are not considered by LDA, such as users, hashtags, mentions, etc.\n\n\n\n\n\nMerge all short documents by the same users into a long document\nUse a single topic for each short document\n\nPer-shot-text LDA (aka. Twitter-LDA)\nDirichlet-Multinomial Mixture (DMM)\n\nLearn topics from longer documents (e.g., news articles) and apply them to short texts\nClassify shot texts utilizing neural networks\n\nThe word mover’s distance (WMD)\nWord embeddings\n\nClustering techniques\n\n\n\n\n\nThe user information is lost\nThe co-occurence of words within the same short texts is lost\nRequires a corpus of long text that is a priori compatible with the short-text documents\nRely on pre-trained word embeddings may not reflect the specific voabulary and semantic usage of words in the short texts\n\n\n\n\n\nShort text LDA topic model with unsupervised clustering of authors of short documents\nFuses the clustering of both authors(users) and documents\nThis hierachical model is able to share information at multiple levels leading to higher quality estimates of per-author topic distributions, per-cluster topic distribution centers, and autho cluster assignments."
  },
  {
    "objectID": "index.html#part-2-the-stlda-c-model",
    "href": "index.html#part-2-the-stlda-c-model",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "Part 2: The stLDA-C Model",
    "text": "Part 2: The stLDA-C Model\nThe stLDA-C model is specifically designed to handle the sparsity of words in short texts by considering the additional structure provided by user clusters and potentially by integrating external information or employing different priors that are more suitable for short texts.\nFrom traditional\n\n\n\nLDA\n\n\nto\n\n\n\nstLDA-C\n\n\nvisualization is under construction\nThree most important addtions to the traditional LDA model:\n\nUser Clustering: This model introduces a layer of user clustering that groups users into different clusters based on their topic preferences.\nHierarchical Topic Distributions: It incorporates hierarchical priors, which allow for cluster-level topic distributions and user-specific topic distributions."
  },
  {
    "objectID": "index.html#part-3-the-stlda-c-model-in-action-analyze-us-senators-tweets-from-a-single-day",
    "href": "index.html#part-3-the-stlda-c-model-in-action-analyze-us-senators-tweets-from-a-single-day",
    "title": "stLDA-C: A Topic Model for Short Texts",
    "section": "Part 3: The stLDA-C Model in Action: Analyze US Senators’ tweets from a single day",
    "text": "Part 3: The stLDA-C Model in Action: Analyze US Senators’ tweets from a single day\n\n3.1 Visualize the networks of tweets and users\n\n\nCode\n# Create the networks\ntweets_w &lt;- PrepText(tweets, groupvar = \"screen_name\", textvar = \"text\", node_type = \"words\", tokenizer = \"words\", pos = \"nouns\", remove_stop_words = TRUE, compound_nouns = TRUE)\ntweets_g &lt;- PrepText(tweets, groupvar = \"screen_name\", textvar = \"text\", node_type = \"groups\", tokenizer = \"words\", pos = \"nouns\", remove_stop_words = TRUE, compound_nouns = TRUE)\n\ntweets_w_nw &lt;- CreateTextnet(tweets_w)\ntweets_g_nw &lt;- CreateTextnet(tweets_g)\n\n# Save the networks to local files\nsaveRDS(tweets_w_nw, \"rcs/data/tweets_w_nw.rds\")\nsaveRDS(tweets_g_nw, \"rcs/data/tweets_g_nw.rds\")\n\n# # ## Check the distribution of degree of the nodes\n# degree &lt;- degree(tweets_w_nw)\n# hist(degree, breaks = 100, main = \"Degree Distribution of Words\", xlab = \"Degree\")\n# # \n# # ## Check number of nodes and edges\n# vcount(tweets_w_nw)\n\nVisTextNet(tweets_w_nw, alpha = 0.25, label_degree_cut=10, betweenness=FALSE)\n\n\n\n\n\nCode\nVisTextNet(tweets_g_nw, alpha = 0.25, label_degree_cut=0, betweenness=TRUE)\n\n\n\n\n\n\n\n3.2 Use stLDA-C to analyze US Senators’ tweets. (One day’s tweets from 100 US Senators)\n\n\nCode\n#######################\n### Visualizations ####\n#######################\n\n#print top 5 words from each topic\ngroundtruth_estimate[[\"tw\"]] %&gt;% \n  top_topic_words(words = words,n=10) %&gt;% \n  t\n\n\n      [,1]     [,2]     [,3]       [,4]           [,5]       [,6]      \n [1,] \"court\"  \"help\"   \"campaign\" \"@brianschatz\" \"campaign\" \"campaign\"\n [2,] \"help\"   \"just\"   \"just\"     \"campaign\"     \"help\"     \"just\"    \n [3,] \"just\"   \"must\"   \"know\"     \"day\"          \"just\"     \"know\"    \n [4,] \"must\"   \"need\"   \"must\"     \"just\"         \"must\"     \"must\"    \n [5,] \"need\"   \"now\"    \"now\"      \"must\"         \"now\"      \"now\"     \n [6,] \"now\"    \"people\" \"people\"   \"now\"          \"people\"   \"people\"  \n [7,] \"people\" \"senate\" \"support\"  \"people\"       \"senate\"   \"thank\"   \n [8,] \"thank\"  \"thank\"  \"thank\"    \"thank\"        \"thank\"    \"trump\"   \n [9,] \"us\"     \"us\"     \"us\"       \"trump\"        \"us\"       \"us\"      \n[10,] \"vote\"   \"vote\"   \"vote\"     \"vote\"         \"vote\"     \"vote\"    \n\n\nCode\n#print cluster means with user-level topic estimates overlayed\n#grey bars are cluster-level expected values, colored lines are each user's topic distribution\n#note that clusters with 1 user do not visualize well\n\n# Extract estimated cluster assignments from the model results\nca_est &lt;- groundtruth_estimate[[\"ca\"]] %&gt;% results_freq_table() %&gt;% apply(1, which.max)\n\n# The following line is commented out because ca_true doesn't exist in your actual data scenario\n# table(ca_est, ca_true)\n\n# Function to plot clusters\nplot_clusters &lt;- function(ut_mat, cluster_assignment, cluster_alphas, yRange = c(0, .5)) {\n  cluster_means &lt;- cluster_alphas %&gt;% {./rowSums(.)}\n  ut_mat &lt;- ut_mat %&gt;% {./rowSums(.)}\n  \n  lapply(unique(cluster_assignment), function(c) {\n    ut_mat %&gt;%\n    {.[cluster_assignment == c, ]} %&gt;%\n      t %&gt;%\n      data.frame(Topic = 1:ncol(ut_mat), .) %&gt;%\n      reshape2::melt(id.vars = \"Topic\") %&gt;%\n      ggplot(aes(x = Topic, y = value)) +\n      geom_line(aes(color = variable)) +\n      guides(color = \"none\") +\n      geom_bar(data = data.frame(x = 1:ncol(ut_mat), y = cluster_means[c, ]), aes(x = x, y = y), alpha = .5, stat = \"identity\") +\n      labs(title = str_c(\"Cluster \", c, \" (n=\", sum(cluster_assignment == c), \")\"), y = \"Probability\") +\n      ylim(yRange)\n  })\n}\n\n# Generate and arrange cluster plots\nclusterPlots &lt;- plot_clusters(ut_mat = groundtruth_estimate[[\"ut\"]] %&gt;% results_array_mean(),\n                              cluster_assignment = groundtruth_estimate[[\"ca\"]] %&gt;% results_freq_table() %&gt;% apply(1, which.max),\n                              cluster_alphas = groundtruth_estimate[[\"alphag\"]] %&gt;% results_array_mean())\n\nclusterPlots %&gt;% gridExtra::grid.arrange(grobs = .)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]