---
title: "stLDA-C: A Topic Model for Short Texts"
title-block-banner: "#041E42"
author: 
  - name: Zhiqiang Ji
    email: zj117@georgetown.edu
    affiliation: 
      - name: McCourt School of Public Policy, Georgetown University
        url: https://mccourt.georgetown.edu/
date: "`r Sys.Date()`"
resources: "rcs/"
format:
  html:
    code-fold: true
    toc: true
    toc_depth: 3
    toc_float: true
    self-contained: true
execute: 
  warning: false
---

## Introduction: Topic Modeling and LDA

**Topic modeling** in Natural Language Processing (NLP) is a technique used to discover hidden themes or topics within a collection of text documents. It's an unsupervised machine learning technique, meaning it doesn't require predefined tags or training data that's been previously classified by humans. The main objective of topic modeling is to discover topics that are clusters of words expressed as a combination of strongly related words.

One popular algorithm for topic modeling is Latent Dirichlet Allocation (LDA). Topic modeling is used in various applications such as chatbots, autocorrection, speech recognition, language translation, social media monitoring, hiring and recruitment, email filtering, and more.

### What's LDA?

Latent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data such as text corpora. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. The topic proportions of a document are assumed to have a Dirichlet prior. The topic-specific word distributions also have a Dirichlet prior.

![LDA Diagram, credit:[Think Infi](https://thinkinfi.com/latent-dirichlet-allocation-for-beginners-a-high-level-overview/)](rcs/LDA_diagram1.png)

LDA has these key assumptions: - Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. - Documents are exchangeable. - Requirements for the data: - Each document must represent a mixture of topics. - Each word must be generated from a single topic.

### What's LDA's problem with short texts like *tweets*?

-   LDA is designed for long documents, but tweets are short.
-   LDA assumes that each document is a mixture of topics, but tweets are often about a single topic.
-   Short texts from social network platform has other characteristics that are not considered by LDA, such as users, hashtags, mentions, etc.

### What are the fixes by far? What are their problems?

-   Merge all short documents by the same users into a long document
-   Use a single topic for each short document
    -   Per-shot-text LDA (aka. [Twitter-LDA](https://github.com/minghui/Twitter-LDA))
    -   Dirichlet-Multinomial Mixture ([DMM](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030126))
-   Learn topics from longer documents (e.g., news articles) and apply them to short texts
-   Classify shot texts utilizing neural networks
    -   The word mover's distance ([WMD](https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html))
    -   Word embeddings
-   Clustering techniques

However, while there are methods available for analyzing short-text documents, they do have some limitations. Specifically, these methods do not retain user information and co-occurrence of words within the same short texts. Additionally, they require a corpus of long text that is already compatible with the short-text documents, and relying on pre-trained word embeddings may not accurately reflect the specific vocabulary and semantic usage of words in the short texts.

![](rcs/tm_levity.png)


## Introducing the stLDA-C Model

The stLDA-C model was proposed by Tierney et al. in their paper "Author Clustering and Topic Estimation for Short Texts." This model particularly aims to improve topic estimation in brief documents, such as social media posts, and incorporates the grouping of authors for more effective analysis.

stLDA features: 
- Short text LDA topic model with unsupervised clustering of authors of short documents 
- Fusing the clustering of **both authors(users) and documents** 
- Hierarchical model capalbe of sharing information at multiple levels leading to higher quality estimates of per-author topic distributions, per-cluster topic distribution centers, and author cluster assignments.

The stLDA-C model is specifically designed to handle the sparsity of words in short texts by considering the additional structure provided by user clusters and potentially by integrating external information or employing different priors that are more suitable for short texts.

To understand what's new in the stLDA-C model, let's first take a closer look at the traditional LDA model.

::: {layout-ncol="2"}
![Traditional LDA](rcs/LDA_math.png)

![stLDA-C](rcs/stLDA_diagram.png)
:::

Quick summary of the traditional LDA notations:

**W**: Word **Z**: Topic

LDA Input: 1. **M** number of documents 2. Each of these documents have **N** number of words

LDA Output: 1. **K** number of topics (cluster of words) 1. **Î¦** distribution (document to topic distribution)

Compared with the traditional LDA, the stLDA-C model adds a layer of user clustering and a layer of hierarchical topic distributions. From the diagram below, we can see that

From the diagrams, we can see that the stLDA-C model introduced several changes and additions:

1.  The model considers $G$ clusters of users, where $G$ is a hyperparameter.
2.  $G_u$ represents the assignment of each user to a specific cluster, governed by the $\phi$ parameter.
3.  $\alpha_g$ is the vector parameter of a Dirichlet distribution over topics choices for users in cluster $g$.
4.  $\phi$ represents the distribution over user clusters. It forms a prior for the user cluster assignments. In traditional LDA, there is no concept of user clusters, so this parameter $\nu$ is specific to stLDA. $\phi$ encodes the proportion of users in each group and forms a prior distribution for $Gu$.
5.  $\theta_u$: Because the model assumes that each document (tweet) is generated by a single topic, the consideration for the document-topic distribution is replaced by user-topic distribution. Each user-specific topic distribution $\theta_u$ is a draw from $Dir(\alpha_g)$, where $g$ is the cluster assignment of user $u$.
6.  $Z_{ud}$ is the topic of each tweet $d$ by user $u$. $Z_{ud}$ is a single draw from $\theta_u$, and all words in tweet $ud$ are sampled from the topic distribution over words, $\beta_t$, where $Z_{ud} = t$.

The generative process of the stLDA-C model is as follows:

![Generative process of the stLDA-C model](rcs/generative.png)

Very intimidating, right? Let's break it down.

![stLDA-C model workflow](rcs/stLDA_canva.png)

Three key takeaways from the stLDA model:

1. **User Clustering**: stLDA clusters users by topic preferences, enhancing the analysis of datasets where authorship is significant.
2. **Hierarchical Topic Distributions**: The model employs hierarchical priors for nuanced cluster-level and user-specific topic analysis.
3. **Integrated Topic-User and Word Analysis**: stLDA combines topic-user dynamics with word co-occurrence for comprehensive short text analysis.

![](rcs/banner.png)

## Part 3: The stLDA-C Model in Action: Analyze US Senators' tweets from a single day

```{r include=FALSE}
# Load the packages
library(devtools)
library(textnets)
library(topicmodels)
library(tidytext)
library(tidyverse)
library(quanteda)
library(tidytext)
library(textdata)
library(networkD3)
library(igraph)

# Load the data
tweets_all <- read.csv("rcs/data/senate_tweets_09.26.2020.csv")
# keep only the text and the user id
tweets <- tweets_all[,c("text", "screen_name", "user_id")]
# keep 20 tweets for each screen_name (scale down to save running time)
tweets <- tweets %>% group_by(screen_name) %>% slice(1:10) %>% ungroup()
# pick 20 screen_name randomly and only keep their tweets, 20 for each screen_name, set seed=196
tweets <- tweets %>% filter(screen_name %in% sample(unique(screen_name), 20, replace = FALSE, set.seed(42)))
```

### 3.1 Visualize the networks of tweets and users

```{r}
# Create the networks
tweets_w <- PrepText(tweets, groupvar = "screen_name", textvar = "text", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
tweets_g <- PrepText(tweets, groupvar = "screen_name", textvar = "text", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)

tweets_w_nw <- CreateTextnet(tweets_w)
tweets_g_nw <- CreateTextnet(tweets_g)

# Save the networks to local files
saveRDS(tweets_w_nw, "rcs/data/tweets_w_nw.rds")
saveRDS(tweets_g_nw, "rcs/data/tweets_g_nw.rds")

# # ## Check the distribution of degree of the nodes
# degree <- degree(tweets_w_nw)
# hist(degree, breaks = 100, main = "Degree Distribution of Words", xlab = "Degree")
# # 
# # ## Check number of nodes and edges
# vcount(tweets_w_nw)

VisTextNet(tweets_w_nw, alpha = 0.25, label_degree_cut=10, betweenness=FALSE)
VisTextNet(tweets_g_nw, alpha = 0.25, label_degree_cut=0, betweenness=TRUE)

```

### 3.2 Use stLDA-C to analyze US Senators' tweets. (One day's tweets from 20 US Senators, 10 tweets per person)

```{r include=FALSE}
# Replicate the demo code from the authors
source("rcs/scripts/setup.R")
source("rcs/scripts/helper_functions.R")
source("rcs/scripts/gibbs_functions.R")

# Generate Document-Feature Matrix from tweets
dfmat_tweets <- dfm(tweets$text, remove_punct = TRUE, remove = stopwords('en')) %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", max_docfreq = 0.1, docfreq_type = "prop")

# Ensure all documents have content
dfmat_tweets <- dfmat_tweets[ntoken(dfmat_tweets) > 0,]

# Convert to a Document-Term Matrix (DTM) for LDA
dtm <- convert(dfmat_tweets, to = "topicmodels")

### LDA Model Fitting ###

# Set the number of topics
nT <- 6  
lda <- LDA(dtm, k = nT, control = list(seed = 42))

# Extract the topic distributions
topics <- tidy(lda, matrix = "beta")

# Generate a topic-word matrix
topics_tw <- topics %>%
  group_by(topic) %>%
  spread(key = topic, value = beta)

words <- topics_tw$term
tw_true <- topics_tw[, 2:(nT + 1)] %>% t

### Clustering ###
# Number of Clusters
nC <- 2  # Adjust this based on your analysis requirement

# Users - using screen names from tweets
users <- tweets$screen_name
dw <- as.matrix(dfmat_tweets)
# Fit the stLDA-C Model
groundtruth_estimate <- collapsed_gibbs_1topic_clusters(alpha = 1, eta = .1, nu = 1,
                                                        users = users, dw = tw_true,  
                                                        nT = nT, nC = nC,
                                                        niter = 50,
                                                        seed = 196, mcmc_update = T,
                                                        nClusterIter = 100,
                                                        mu_scale = 0, sigma_scale = 100,
                                                        prop_scale_center = 100, alphag_sample_method = "componentwise",
                                                        print_clusters = T)
```

```{r}

#######################
### Visualizations ####
#######################

#print top 5 words from each topic
groundtruth_estimate[["tw"]] %>% 
  top_topic_words(words = words,n=10) %>% 
  t

#print cluster means with user-level topic estimates overlayed
#grey bars are cluster-level expected values, colored lines are each user's topic distribution
#note that clusters with 1 user do not visualize well

# Extract estimated cluster assignments from the model results
ca_est <- groundtruth_estimate[["ca"]] %>% results_freq_table() %>% apply(1, which.max)

# The following line is commented out because ca_true doesn't exist in your actual data scenario
# table(ca_est, ca_true)

# Function to plot clusters
plot_clusters <- function(ut_mat, cluster_assignment, cluster_alphas, yRange = c(0, .5)) {
  cluster_means <- cluster_alphas %>% {./rowSums(.)}
  ut_mat <- ut_mat %>% {./rowSums(.)}
  
  lapply(unique(cluster_assignment), function(c) {
    ut_mat %>%
    {.[cluster_assignment == c, ]} %>%
      t %>%
      data.frame(Topic = 1:ncol(ut_mat), .) %>%
      reshape2::melt(id.vars = "Topic") %>%
      ggplot(aes(x = Topic, y = value)) +
      geom_line(aes(color = variable)) +
      guides(color = "none") +
      geom_bar(data = data.frame(x = 1:ncol(ut_mat), y = cluster_means[c, ]), aes(x = x, y = y), alpha = .5, stat = "identity") +
      labs(title = str_c("Cluster ", c, " (n=", sum(cluster_assignment == c), ")"), y = "Probability") +
      ylim(yRange)
  })
}

# Generate and arrange cluster plots
clusterPlots <- plot_clusters(ut_mat = groundtruth_estimate[["ut"]] %>% results_array_mean(),
                              cluster_assignment = groundtruth_estimate[["ca"]] %>% results_freq_table() %>% apply(1, which.max),
                              cluster_alphas = groundtruth_estimate[["alphag"]] %>% results_array_mean())

clusterPlots %>% gridExtra::grid.arrange(grobs = .)





```

## See more:

-   [What is Topic Modeling? Definition, Uses, & Examples](https://dovetail.com/customer-research/topic-modeling/)
-   [What is Topic Modeling? A Beginner's Guide](https://levity.ai/blog/what-is-topic-modeling)
-   [Topic Modeling: An Introduction - MonkeyLearn](https://monkeylearn.com/blog/introduction-to-topic-modeling/)
-   [Topic Modelling \| Topic Modelling in Natural Language Processing.](https://www.analyticsvidhya.com/blog/2021/05/topic-modelling-in-natural-language-processing/)
-   [A Beginner's Guide to Latent Dirichlet Allocation(LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)
